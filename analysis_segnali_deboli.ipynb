{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, date, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import json\n",
    "import re\n",
    "from collections.abc import Iterable\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import TweetsUtils\n",
    "import importlib\n",
    "importlib.reload(TweetsUtils)\n",
    "from TweetsUtils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KEYWORDS = ['febbre', 'brividi', 'dolori alle ossa', 'dolori muscolari', 'malessere generale', \n",
    "#             'mal di testa', 'mal di gola', 'raffreddore', 'tosse', 'congiuntivite']\n",
    "KEYWORDS = ['febbre', 'mal di testa', 'raffreddore', 'mal di gola', 'tosse']\n",
    "COVID_KEYWORDS = ['gusto', 'olfatto']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = 'tweets/flu_tweets/'\n",
    "tweets_filename = base_path + 'flu_tweets_contents.json'\n",
    "users_filename = base_path + 'flu_tweets_users.json'\n",
    "places_filename = base_path + 'flu_tweets_places.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweets = read_file(tweets_filename)\n",
    "for tw in tweets:\n",
    "    tw['datetime'] = datetime.strptime(tw['datetime'], '%Y-%m-%d %H:%M:%S')\n",
    "users = read_file(users_filename)\n",
    "places = read_file(places_filename)\n",
    "\n",
    "print(len(tweets), 'tweets')\n",
    "print(len(users), 'users')\n",
    "print(len(places), 'places')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out tweets without keywords\n",
    "# tweets = [tw for tw in tweets if count_keywords(tw['text'].lower(), KEYWORDS+COVID_KEYWORDS) > 0] # <-------- CAMBIARE SE SERVE\n",
    "tweets = [tw for tw in tweets if count_keywords(tw['text'].lower(), KEYWORDS) > 0] # <-------- CAMBIARE SE SERVE\n",
    "print('removed tweets without keywords:', len(tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out tweets of users that tweet too much (more than 50 tweets about flu)\n",
    "x = pd.value_counts(select_fields(tweets, ['author_id'], as_list=True))\n",
    "x = x[x>=50]\n",
    "ids = list(x.index)\n",
    "tweets = [tw for tw in tweets if tw['author_id'] not in ids]\n",
    "print('removed \"super\" tweeters:', len(tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out tweets with popular mentions (over 50 times)\n",
    "x = select_fields(tweets, ['text'], as_list=True)\n",
    "x = [get_mentions(text) for text in x]\n",
    "x = [item.lower() for sublist in x for item in sublist] #flatten\n",
    "x = pd.value_counts(x)\n",
    "x = x[x>50]\n",
    "x = set(x.index)\n",
    "tweets = [tw for tw in tweets if not has_mentions(tw['text'], x)]\n",
    "print('removed popular mentions:', len(tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # filter out ambiguous tweets (brividi in combination with at least another keyword)\n",
    "# tweets_brividi = keyword_in_combination(tweets, 'brividi', KEYWORDS, 2)\n",
    "# tweets = [tw for tw in tweets if 'brividi' not in tw['text'].lower()]\n",
    "# tweets = tweets + tweets_brividi\n",
    "# print('removed ambiguous keywords:', len(tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out tweets with popular hashtags (more than 100 times, excluding some specific ones)\n",
    "x = select_fields(tweets, ['hashtags'], as_list=True)\n",
    "x = list(np.array(x, dtype='object').reshape(-1))\n",
    "x = [h[0][0].lower() for h in x if len(h) > 0]\n",
    "x = pd.value_counts(x)\n",
    "x = list(x[x>100].index)\n",
    "x = [h for h in x if (h not in KEYWORDS) \n",
    "     and (h not in ['influenza', 'salute', 'luned√¨', 'primavera', 'malditesta', 'emicrania', 'benemanonbenissimo']) \n",
    "     and ('buon' not in h) and ('febbre' not in h)] \n",
    "x = [h for h in x if ('vaccin' not in h) and ('virus' not in h) and ('covid' not in h) and ('corona' not in h)]\n",
    "tweets = [tw for tw in tweets if not has_words(tw['text'], x)]\n",
    "print('removed popular hashtags:', len(tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out outliers\n",
    "def outlier_in_text(text):\n",
    "    for outlier in ['alessiamorani', 'alessia morani', 'morani', 'temptationisland', 'benji_mascolo', \n",
    "                    'higuain', 'milan', 'arisa', 'sanremo', 'claudio', 'clario', 'gf16']:\n",
    "        if outlier in text:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "tweets = [tw for tw in tweets if not outlier_in_text(tw['text'].lower())]\n",
    "print('removed outliers:', len(tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_tweets = tweets.copy()\n",
    "tweets = filter_list(tweets, 'datetime', datetime(2017,1,1), datetime(2020,1,20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**statistiche**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = [[k, len([tw for tw in full_tweets if k in tw['text'].lower()])] for k in KEYWORDS]\n",
    "pd.DataFrame(tmp, columns=['keyword', 'count']).sort_values(by='count', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (8,5)\n",
    "\n",
    "x = pd.DataFrame(select_fields(full_tweets, ['datetime', 'text']))\n",
    "x['year'] = x['datetime'].dt.year\n",
    "x['month'] = x['datetime'].dt.month\n",
    "del x['datetime']\n",
    "x = x.groupby(['year', 'month']).count().reset_index()\n",
    "\n",
    "i1 = 0\n",
    "i2 = 255\n",
    "for year in [2017,2018,2019,2020,2021,2022]:\n",
    "    tmp = x[x['year']==year].copy()\n",
    "#     print(year, np.sum(tmp['text']))\n",
    "    col = (i1/255, 0.25, i2/255)\n",
    "    plt.plot(list(tmp['month']), list(tmp['text']), label=year, color=col)\n",
    "    i1 += 50\n",
    "    i2 -= 50\n",
    "    \n",
    "plt.legend(loc='upper center')\n",
    "# plt.tight_layout()\n",
    "# plt.savefig('timeseries_by_year.svg')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_fields(random.sample(tweets, 10), ['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweets quantity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linechart(s, rolling_mean=None, normalize=False):\n",
    "    if rolling_mean:\n",
    "        s = s.rolling(window=rolling_mean, center=True).mean()\n",
    "    if normalize:\n",
    "        s = (s - s.min()) / (s.max() - s.min())\n",
    "    plt.plot(s)\n",
    "\n",
    "\n",
    "def get_tweets_volume(tweets, resampling='1d'):\n",
    "    s = pd.DataFrame(index=pd.to_datetime([tw['datetime'] for tw in tweets]))\n",
    "    s['count'] = [1]*len(s)\n",
    "    s = s.resample(resampling).count()\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (9,5)\n",
    "\n",
    "# s = get_tweets_volume(tweets, resampling='1d')\n",
    "# linechart(s, rolling_mean=1)\n",
    "# plt.xlim(datetime(2017,1,1), datetime(2022,7,1))\n",
    "# plt.ylim(50,840)\n",
    "# plt.savefig('half_timeseries.jpg', bbox_inches='tight')\n",
    "# plt.show();\n",
    "\n",
    "# s = get_tweets_volume(full_tweets, resampling='1d')\n",
    "# linechart(s, rolling_mean=1)\n",
    "# plt.xlim(datetime(2017,1,1), datetime(2022,7,1))\n",
    "# plt.ylim(50,840)\n",
    "# plt.savefig('full_timeseries.jpg', bbox_inches='tight')\n",
    "# plt.show();\n",
    "\n",
    "s = get_tweets_volume(full_tweets, resampling='1d')\n",
    "plt.plot(s)\n",
    "tmp = filter_list(full_tweets, 'datetime', datetime(2020,1,20), datetime(2022,10,20))\n",
    "s = get_tweets_volume(tmp, resampling='1d')\n",
    "plt.plot(s)\n",
    "plt.xlim(datetime(2017,1,1), datetime(2022,7,1))\n",
    "plt.ylim(50,840)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('full_timeseries.svg')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**salvataggio serie storica**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # filter out tweets without keywords\n",
    "# tmp = [tw for tw in full_tweets if count_keywords(tw['text'].lower(), ['perdita', 'perso']) > 0]\n",
    "# tmp = [tw for tw in tmp if count_keywords(tw['text'].lower(), ['gusto', 'olfatto']) > 0]\n",
    "# print('len:', len(tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # GIORNALIERA ITALIA\n",
    "# df = pd.DataFrame()\n",
    "# for k in KEYWORDS:\n",
    "#     tmp = [tw for tw in full_tweets if k in tw['text'].lower()]\n",
    "#     df[k] = get_tweets_volume(tmp, resampling='1d')\n",
    "# for k in COVID_KEYWORDS:\n",
    "#     tmp = [tw for tw in full_tweets if count_keywords(tw['text'], ['perdita', 'perso']) > 0]\n",
    "#     tmp = [tw for tw in tmp if count_keywords(tw['text'], [k]) > 0]\n",
    "#     df[k] = get_tweets_volume(tmp, resampling='1d')\n",
    "# df['total'] = df.apply(lambda x: np.sum(x), axis=1)\n",
    "# df['date'] = df.index\n",
    "# df = df.fillna(0.0)\n",
    "# df = df[['date']+KEYWORDS+COVID_KEYWORDS+['total']]\n",
    "# df.to_csv('./files/twitter_daily_italy.csv', sep=';', index=False)\n",
    "\n",
    "# # SETTIMANALE ITALIA\n",
    "# df = pd.DataFrame()\n",
    "# for k in KEYWORDS:\n",
    "#     tmp = [tw for tw in full_tweets if k in tw['text'].lower()]\n",
    "#     df[k] = get_tweets_volume(tmp, resampling='1w')\n",
    "# for k in COVID_KEYWORDS:\n",
    "#     tmp = [tw for tw in full_tweets if count_keywords(tw['text'], ['perdita', 'perso']) > 0]\n",
    "#     tmp = [tw for tw in tmp if count_keywords(tw['text'], [k]) > 0]\n",
    "#     df[k] = get_tweets_volume(tmp, resampling='1w')\n",
    "# df['total'] = df.apply(lambda x: np.sum(x), axis=1)\n",
    "# df['date'] = df.index #[day-timedelta(days=6) for day in list(tmp.index)]\n",
    "# df = df.fillna(0.0)\n",
    "# df = df[['date']+KEYWORDS+COVID_KEYWORDS+['total']]\n",
    "# df.to_csv('./files/twitter_weekly_italy.csv', sep=';', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regione = 'sicilia'\n",
    "\n",
    "df = get_tweets_with_location(full_tweets, users, places)\n",
    "df = df[df['region']==regione]\n",
    "tmp_tweets = df.to_dict('records')\n",
    "\n",
    "# GIORNALIERA REGIONE\n",
    "df = pd.DataFrame()\n",
    "for k in KEYWORDS:\n",
    "    tmp = [tw for tw in tmp_tweets if k in tw['text'].lower()]\n",
    "    df[k] = get_tweets_volume(tmp, resampling='1d')\n",
    "for k in COVID_KEYWORDS:\n",
    "    tmp = [tw for tw in tmp_tweets if count_keywords(tw['text'], ['perdita', 'perso']) > 0]\n",
    "    tmp = [tw for tw in tmp if count_keywords(tw['text'], [k]) > 0]\n",
    "    df[k] = get_tweets_volume(tmp, resampling='1d')\n",
    "df['total'] = df.apply(lambda x: np.sum(x), axis=1)\n",
    "df['date'] = df.index\n",
    "df = df.fillna(0.0)\n",
    "df = df[['date']+KEYWORDS+COVID_KEYWORDS+['total']]\n",
    "df.to_csv('./files/twitter_daily_'+regione+'.csv', sep=';', index=False)\n",
    "\n",
    "# SETTIMANALE REGIONE\n",
    "df = pd.DataFrame()\n",
    "for k in KEYWORDS:\n",
    "    tmp = [tw for tw in tmp_tweets if k in tw['text'].lower()]\n",
    "    df[k] = get_tweets_volume(tmp, resampling='1w')\n",
    "for k in COVID_KEYWORDS:\n",
    "    tmp = [tw for tw in tmp_tweets if count_keywords(tw['text'], ['perdita', 'perso']) > 0]\n",
    "    tmp = [tw for tw in tmp if count_keywords(tw['text'], [k]) > 0]\n",
    "    df[k] = get_tweets_volume(tmp, resampling='1w')\n",
    "df['total'] = df.apply(lambda x: np.sum(x), axis=1)\n",
    "df['date'] = df.index #[day-timedelta(days=6) for day in list(tmp.index)]\n",
    "df = df.fillna(0.0)\n",
    "df = df[['date']+KEYWORDS+COVID_KEYWORDS+['total']]\n",
    "df.to_csv('./files/twitter_weekly_'+regione+'.csv', sep=';', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**per analizzare quali tweet portano a picchi improvvisi**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.DataFrame(index=pd.to_datetime([tw['datetime'] for tw in full_tweets]))\n",
    "s['count'] = [1]*len(s)\n",
    "s = s.resample('1d').count()\n",
    "s = s.sort_values(by='count', ascending=False).head(20)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "d1 = list(s.index)[i]\n",
    "d2 = list(s.index)[i]+timedelta(days=1)\n",
    "x = filter_list(full_tweets, 'datetime', d1, d2)\n",
    "x = select_fields(x, ['text'], as_list=True)\n",
    "x = ' '.join(x)\n",
    "pd.value_counts(x.split()).head(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geolocalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_tweets_with_location(full_tweets, users, places)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = len(df[df['type']=='geolocalization'])\n",
    "print('Geolocalization:', l, '(' + str(np.round(100*l/len(full_tweets), 2)) + '%)')\n",
    "\n",
    "l = len(df[df['type']=='user_location'])\n",
    "print('User location:', l, '(' + str(np.round(100*l/len(full_tweets), 2)) + '%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (7,5)\n",
    "\n",
    "x1 = pd.DataFrame(pd.value_counts(df['region']))\n",
    "x1.columns = ['tweets_count']\n",
    "\n",
    "x2 = json.load(open('files/italy_regions_population.json', 'r'))\n",
    "x2 = pd.DataFrame({'population': list(x2.values())}, index=list(x2.keys()))\n",
    "x2['population'] /= 500\n",
    "\n",
    "x = x1.join(x2)\n",
    "\n",
    "fig = x.plot(kind='bar', color=['blue', 'orange'])\n",
    "plt.tight_layout()\n",
    "plt.savefig('regions.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split text for visualization\n",
    "df['formatted_text'] = df['text'].str.wrap(50).apply(lambda x: x.replace('\\n', '<br>'))\n",
    "\n",
    "# re-group date\n",
    "group_by = 'month'\n",
    "code = {'week': 'W', 'month': 'M', 'year': 'Y'}\n",
    "df['datetime'] = df['datetime'].dt.to_period(code[group_by]).apply(lambda r: r.start_time)    \n",
    "df['datetime'] = df['datetime'].astype(str)\n",
    "    \n",
    "# sort by date\n",
    "df = df.sort_values(by='datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.density_mapbox(df, lat=df['lat'], \n",
    "                            lon=df['lon'], \n",
    "                            radius=10,\n",
    "                            hover_data={'formatted_text': True, 'name': True, 'lat': False, 'lon': False, 'datetime': False},\n",
    "                            animation_frame='datetime', \n",
    "                            width=600, height=600\n",
    "                       )\n",
    "fig.update_layout(mapbox_style=\"carto-positron\", mapbox_zoom=4.5, mapbox_center={\"lat\": 42, \"lon\": 12.5})\n",
    "fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "\n",
    "fig.layout.updatemenus[0].buttons[0].args[1][\"frame\"][\"duration\"] = 600\n",
    "fig.layout.updatemenus[0].buttons[0].args[1][\"transition\"][\"duration\"] = 600\n",
    "fig.layout.coloraxis.showscale = True   \n",
    "fig.layout.sliders[0].pad.t = 10\n",
    "fig.layout.updatemenus[0].pad.t= 10\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df[['datetime', 'region', 'text']].groupby(['datetime', 'region']).count().unstack(fill_value=0).stack()\n",
    "x = x.reset_index()\n",
    "x.columns = [group_by, 'region', 'count']\n",
    "pop = read_file('files/italy_regions_population.json')\n",
    "x['normalized_count'] = x.apply(lambda x: x['count']/pop[x['region']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.displot(x['normalized_count'], bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### https://github.com/deldersveld/topojson\n",
    "### then converted from topojson to geojson\n",
    "italy_regions_geo = read_file('files/italy_regions_borders.geojson')\n",
    "\n",
    "# Choropleth representing the length of region names\n",
    "fig = px.choropleth(data_frame=x, \n",
    "                    geojson=italy_regions_geo, \n",
    "                    locations='region', # name of dataframe column\n",
    "                    featureidkey='properties.NAME_1',  # path to field in GeoJSON feature object with which to match the values passed in to locations\n",
    "                    color='normalized_count',\n",
    "                    color_continuous_scale=\"ylorbr\",\n",
    "                    animation_frame=group_by,\n",
    "                    scope=\"europe\",\n",
    "                    range_color=(0,0.00006),#max(x['normalized_count'])),\n",
    "                    width=1440, height=900\n",
    "                   )\n",
    "fig.update_geos(showcountries=False, showcoastlines=False, showland=True, fitbounds=\"locations\")\n",
    "fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0}, dragmode=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confronti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**twitter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = get_tweets_volume(full_tweets, resampling='7d')\n",
    "t = t['count']\n",
    "t = t.apply(lambda x: (x-np.mean(t))/np.std(t)) #standardize\n",
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**google**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# google\n",
    "g = pd.read_csv('./files/google_weekly_italy.csv', sep=';')\n",
    "g.index = pd.to_datetime(g['date'])\n",
    "del g['date']\n",
    "# g = g['2017-01-01':'2022-05-15']\n",
    "g = g['average']\n",
    "g = g.apply(lambda x: (x-np.mean(g))/np.std(g)) #standardize\n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**influweb** (https://influenzanet.info/#page/home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "off = pd.read_csv('./files/IT_incidence.csv')\n",
    "off = off[off['syndrome']>='ili.ecdc']\n",
    "off['year'] = off['yw'].astype(str).str[:4].astype(int)\n",
    "off['week'] = off['yw'].astype(str).str[-2:].astype(int)\n",
    "off = off[off['year']>=2017]\n",
    "off = off[['year', 'week', 'incidence', 'lower', 'upper', 'count', 'part']].reset_index(drop=True)\n",
    "off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "all_years = list(range(2017,2022+1))\n",
    "all_weeks = list(range(1,52+1))\n",
    "combined = [all_years, all_weeks]\n",
    "df1 = pd.DataFrame(columns = ['year', 'week'], data=list(itertools.product(*combined)))\n",
    "off = df1.merge(off, how='left', left_on=['year', 'week'], right_on=['year', 'week']).copy().fillna(0)\n",
    "off = off.iloc[:len(g)]['count']\n",
    "off.index = g.index\n",
    "off = off.apply(lambda x: (x-np.mean(off))/np.std(off)) #standardize\n",
    "off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**flunet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # https://www.who.int/tools/flunet\n",
    "# filename = './files/FluNetInteractiveReport.csv'\n",
    "\n",
    "# official = pd.read_csv(filename)\n",
    "# for field in ['Country', 'WHOREGION', 'FLUREGION']:\n",
    "#     del official[field]\n",
    "\n",
    "# c = official.columns\n",
    "# official = official[list(c[:5]) + list(c[-3:])]\n",
    "# official.columns = ['year', 'week', 'start_date', 'end_date', 'number_specimen', \n",
    "#                     'all_positive_viruses', 'all_negative_viruses', 'activity']\n",
    "\n",
    "# official = official.fillna(0)\n",
    "\n",
    "# tmp = official[['start_date', 'number_specimen']].copy()\n",
    "# tmp = tmp.set_index('start_date')\n",
    "# tmp.index = pd.to_datetime(tmp.index)\n",
    "# tmp.index = [t+timedelta(-1) for t in tmp.index]\n",
    "\n",
    "# tmp = tmp['2017-01-01':'2022-05-15']\n",
    " \n",
    "# tmp = (tmp - tmp.min()) / (tmp.max() - tmp.min()) #normalize\n",
    "\n",
    "# tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**plottini**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (10,5)\n",
    "\n",
    "tmp = pd.concat([t,g,off],axis=1)\n",
    "tmp.columns = ['twitter', 'google', 'official']\n",
    "\n",
    "tmp1 = pd.concat([t,off],axis=1)\n",
    "tmp1.columns = ['twitter', 'official']\n",
    "\n",
    "tmp2 = pd.concat([g,off],axis=1)\n",
    "tmp2.columns = ['google', 'official']\n",
    "\n",
    "tmp3 = pd.concat([t,g],axis=1)\n",
    "tmp3.columns = ['twitter', 'google']\n",
    "\n",
    "# tmp3 = tmp.copy()\n",
    "# tmp3['avg'] = tmp3.apply(lambda x: np.mean([x['twitter'], x['google']]), axis=1)\n",
    "# tmp3 = tmp3[['avg', 'official']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (9,5)\n",
    "\n",
    "for df in [tmp1, tmp2, tmp3]:\n",
    "    \n",
    "    if 'official' in df.columns:\n",
    "        mse = mean_squared_error(df['official'], df.iloc[:,0])\n",
    "        print(list(df.columns), '-->', np.round(mse, 3))\n",
    "    \n",
    "    for col in df.columns:\n",
    "    \n",
    "        if col == 'twitter':\n",
    "            color = 'C0'\n",
    "        elif col == 'google':\n",
    "            color = 'C1'\n",
    "        elif col == 'avg':\n",
    "            color = 'C3'\n",
    "        else:\n",
    "            color = 'green'\n",
    "\n",
    "        plt.plot(df[col], color=color, label=col)\n",
    "    \n",
    "    plt.legend(loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('confronto_'+('_'.join(list(df.columns)))+'.svg')\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = t['2017-07-01':'2019-06-01'].copy()\n",
    "t2[t2 < 0] = 0\n",
    "\n",
    "g2 = g['2017-07-01':'2019-06-01'].copy()\n",
    "g2[g2 < 0] = 0\n",
    "\n",
    "off2 = off['2017-07-01':'2019-06-01'].copy()\n",
    "off2[off2 < 0] = 0\n",
    "\n",
    "plt.plot(t2)\n",
    "plt.plot(g2)\n",
    "plt.plot(off2)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = t2['2017-07-01':'2018-06-01'].copy()\n",
    "t_twitter = list(tmp[tmp>0].index)[0]\n",
    "\n",
    "tmp = g2['2017-07-01':'2018-06-01'].copy()\n",
    "t_google = list(tmp[tmp>0].index)[0]\n",
    "\n",
    "tmp = off2['2017-07-01':'2018-06-01'].copy()\n",
    "t_official = list(tmp[tmp>0].index)[0]\n",
    "\n",
    "print('     twitter     |      google')\n",
    "print(t_official - t_twitter, '|', t_official - t_google)\n",
    "\n",
    "tmp = t2['2018-07-01':'2019-06-01'].copy()\n",
    "t_twitter = list(tmp[tmp>0].index)[0]\n",
    "\n",
    "tmp = g2['2018-07-01':'2019-06-01'].copy()\n",
    "t_google = list(tmp[tmp>0].index)[0]\n",
    "\n",
    "tmp = off2['2018-07-01':'2019-06-01'].copy()\n",
    "t_official = list(tmp[tmp>0].index)[0]\n",
    "\n",
    "print(t_official - t_twitter, '|', t_official - t_google)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
